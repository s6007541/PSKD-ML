# Progressive-Self-Knowledge-Distillation-with-Mutual-Learning

Deep neural networks based on large network architec- tures are often prone to the over-fitting problem and thus in- adequate for generalization. Recent self-knowledge distilla- tion (self-KD) approaches have successfully addressed this issue by regularizing a single network using dark knowledge (e.g., knowledge acquired from wrong predictions). Moti- vated by the idea of online collaborative learning using a large student cohort, we extend the online self-KD methods by combining the two learning schemes as in real-world learning environments. We seek to mimic the real-world self- and collaborative-learning strategies in deep neural networks for the image classification task, aimed to bet- ter predict the classification accuracy with lower computa- tional costs during training. We closely explore the perfor- mance of a teacher-free dynamically evolving self-distilled network and verify that our approach on the CIFAR-100 dataset gives sufficient insights into combining self-KD and mutual feature learning.
